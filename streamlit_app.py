import traceback

import streamlit as st
from pathlib import Path

from retriever.vectorize_files import vectorize_uploaded_files
from qa_chain.get_qa_history_chain import get_qa_history_chain
from qa_chain.get_response import gen_response


# Streamlit åº”ç”¨ç¨‹åºç•Œé¢
def main():
    st.markdown('### ğŸ¦œğŸ”— Buddy-AI')
    
    # ä½¿ç”¨ä¾§è¾¹æ æ·»åŠ çŸ¥è¯†åº“ç®¡ç†åŠŸèƒ½
    with st.sidebar:
        st.header("ğŸ“š çŸ¥è¯†åº“ç®¡ç†")

        # åˆ›å»ºçŸ¥è¯†åº“ç›®å½•
        knowledge_db_path = Path("data_base/knowledge_db/data")
        knowledge_db_path.mkdir(parents=True, exist_ok=True)

        # æ–‡ä»¶ä¸Šä¼ 
        uploaded_file = st.file_uploader(
            "ä¸Šä¼ çŸ¥è¯†åº“æ–‡ä»¶",
            type=['pdf', 'docx', 'txt', 'md', 'csv'],
            accept_multiple_files=False,
            help="æ”¯æŒ PDF, DOCX, TXT, MD, CSV æ ¼å¼ï¼Œå•ä¸ªæ–‡ä»¶ä¸è¶…è¿‡ 5MB"
        )

        if uploaded_file is not None:
            # æ£€æŸ¥æ–‡ä»¶å¤§å°
            if uploaded_file.size > 5 * 1024 * 1024:  # 5MB
                st.error("æ–‡ä»¶å¤§å°è¶…è¿‡ 5MB é™åˆ¶")
            else:
                # ä¿å­˜ä¸Šä¼ çš„æ–‡ä»¶
                file_path = knowledge_db_path / uploaded_file.name

                with open(file_path, "wb") as f:
                    f.write(uploaded_file.getvalue())

                st.success(f"æ–‡ä»¶å·²ä¿å­˜: {uploaded_file.name}")

                # å‘é‡åŒ–æŒ‰é’®
                if st.button("ğŸ”¬ å‘é‡åŒ–"):
                    try:
                        with st.spinner("æ­£åœ¨å‘é‡åŒ–æ–‡ä»¶..."):
                            success = vectorize_uploaded_files([str(file_path)])
                            if success:
                                st.success("å‘é‡åŒ–å®Œæˆï¼")
                            else:
                                st.error("å‘é‡åŒ–å¤±è´¥")
                    except Exception as e:
                        st.error(f"å‘é‡åŒ–è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
                        print(traceback.format_exc())

    # ç”¨äºè·Ÿè¸ªå¯¹è¯å†å²
    if "messages" not in st.session_state:
        st.session_state.messages = []
    # å­˜å‚¨æ£€ç´¢é—®ç­”é“¾
    if "qa_history_chain" not in st.session_state:
        st.session_state.qa_history_chain = get_qa_history_chain()

    messages = st.container(height=800)
    # æ˜¾ç¤ºæ•´ä¸ªå¯¹è¯å†å²
    for message in st.session_state.messages:
        with messages.chat_message(message[0]):
            st.write(message[1])
    if prompt := st.chat_input("Say something"):
        # å°†ç”¨æˆ·è¾“å…¥æ·»åŠ åˆ°å¯¹è¯å†å²ä¸­
        st.session_state.messages.append(("human", prompt))
        with messages.chat_message("human"):
            st.write(prompt)

        answer = gen_response(
            chain=st.session_state.qa_history_chain,
            input=prompt,
            chat_history=st.session_state.messages
        )
        with messages.chat_message("ai"):
            output = st.write_stream(answer)
        st.session_state.messages.append(("ai", output))


if __name__ == "__main__":
    main()
